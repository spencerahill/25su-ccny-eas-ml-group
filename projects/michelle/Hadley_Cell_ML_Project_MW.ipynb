{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8a7b6-b29c-41b0-9af6-be14e2d47390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBJECTIVE 1: Training a model that predicts hadley cell shared ascending edge based on streamfunction (input) and labeled latitude of\n",
    "#              the shared ascending edge (output)\n",
    "# \n",
    "# STEP 1: Gather data\n",
    "#         INPUT= Streamfunc with shape: (time: 540  level: 37  lat: 721) - 3D\n",
    "#         OUTPUT= Latitude of shared ascending edge, labeled, with shape: (time: 540) - 1D\n",
    "#\n",
    "# STEP 2: Shape Data\n",
    "#         For NN - 1D vector\n",
    "#         For CNN - 2D matrix\n",
    "#\n",
    "# STEP 3: Normalize the Data (Better performance for NNs)\n",
    "#         1- Take zonal mean of streamfunc\n",
    "#         2- Find std dev of streamfunc\n",
    "#         3- Normalize - streamfunc_norm = (streamfunc - streamfunc_mean) / streamfunc_std\n",
    "#         4- Perform the same action on the labels to ensure values are lebeled correctly\n",
    "#\n",
    "# STEP 4: Define classes\n",
    "#         Define custom PyTorch Dataset class to feed data into NN for training\n",
    "#\n",
    "# STEP 5: Split data into training, validation and testing\n",
    "#\n",
    "# STEP 6: Choose model architecture based on type of problem\n",
    "#         Regression Problem - Simple Feedforward Neural Network\n",
    "#\n",
    "# STEP 7: Initialize model, loss, optimizer\n",
    "#         Model type: Feedforward NN\n",
    "#         Loss Function: MSE\n",
    "#         Optimizer method: Adam\n",
    "#\n",
    "#         STEP 8: Train model (using 60% of data)\n",
    "#\n",
    "#         STEP 9: Validate model with loss function (using 30% of data)\n",
    "#         \n",
    "#         STEP 10: Test model (using 20% of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac57f33-510b-42ef-9d96-2bc7acc9f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "file_path= \"C:/Users/socce/OneDrive/Documents/Desktop/Graduate_Studies/Thesis/hc-edges-and-streamfunc_monthly-ts_era5_1979-2023.nc\"\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "streamfunc = ds[\"streamfunc\"].values  # shape: (time, level, lat)\n",
    "labels = ds[\"hc_edge_shared\"].values   # shape: (time,)\n",
    "\n",
    "#Normalize input and labels\n",
    "streamfunc_mean = np.mean(streamfunc)\n",
    "streamfunc_std = np.std(streamfunc)\n",
    "streamfunc_norm = (streamfunc - streamfunc_mean) / streamfunc_std\n",
    "\n",
    "labels_mean = np.mean(labels)\n",
    "labels_std = np.std(labels)\n",
    "labels_norm = (labels - labels_mean) / labels_std\n",
    "\n",
    "# 3. Define custom dataset\n",
    "class HadleyDataset(Dataset): # Creating a custom class named HadleyDataset that inherits from PyTorch's base Dataset class.\n",
    "    \n",
    "    # PyTorch expects datasets to be in a format it can load in mini-batches, via a DataLoader.\n",
    "    # To do this, subclass torch.utils.data.Dataset and use __init__ , __len__ , and __getitem__\n",
    "    \n",
    "    def __init__(self, X, y): # Specifying how to load/store data\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).reshape(len(X), -1)  # X is 3D normalized stream function data of shape (N_time, levels, latitudes) → e.g. (540, 37, 721)\n",
    "                                                                           # reshape X into a 2D array with shape (N_samples, input_dim) input_dim = 37 × 721 = 26677 (FLATTEN)\n",
    "                                                                           # This makes it suitable for a feedforward neural net, which expects vectors as input\n",
    "\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # y is 1D normalized label array with shape (540) (ascending edge latitude per time step)\n",
    "                                                                    # Reshape it into shape (540, 1) using unsqueeze(1) so it’s compatible with the model’s expected output shape\n",
    "                                                                    # Now self.X is a tensor of shape (N, 26677) & self.y is a tensor of shape (N, 1)\n",
    "    \n",
    "    def __len__(self): # Specifying how many total samples there are\n",
    "        return len(self.y)                                              # Returns the total number of samples (e.g. 540 time steps).\n",
    "                                                                        # This tells PyTorch how many times to iterate through the dataset during training.\n",
    "\n",
    "    def __getitem__(self, idx): # Specifying how to access sample \n",
    "        return self.X[idx], self.y[idx]                                 # When PyTorch loops through the dataset, this returns the input (X[idx]) and label (y[idx]) for the sample at index idx.\n",
    "                                                                        # This will help enable batch processing\n",
    "\n",
    "# 4. Train/val/test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    streamfunc_norm, labels_norm, test_size=0.4, random_state=42\n",
    ")\n",
    "# Splitting streamfunc_norm into training dataset and temporary dataset which will be \n",
    "#           split again later into validation and testing datasets\n",
    "#X_train, y_train: 60% of the data → used to train the model\n",
    "#X_temp, y_temp: remaining 40% → to be split again into val and test\n",
    "#random_state=42: makes the split reproducible every time you run the code\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "#Now you're splitting that 40% temporary set into:\n",
    "#X_val, y_val: 20% of total data → used to validate the model during training (tuning)\n",
    "#X_test, y_test: 20% of total data → used to test the model after training\n",
    "train_dataset = HadleyDataset(X_train, y_train)\n",
    "val_dataset = HadleyDataset(X_val, y_val)\n",
    "test_dataset = HadleyDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 5. Define the model\n",
    "class HadleyNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HadleyNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = streamfunc_norm.shape[1] * streamfunc_norm.shape[2]  # 37 x 721\n",
    "model = HadleyNet(input_dim)\n",
    "\n",
    "# 6. Training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 50\n",
    "\n",
    "# 7. Training loop\n",
    "print(\"Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# 8. Save model and normalization stats\n",
    "torch.save(model.state_dict(), \"hadley_model.pth\")\n",
    "np.savez(\"hadley_norm_stats.npz\", input_mean=streamfunc_mean, input_std=streamfunc_std,\n",
    "         label_mean=labels_mean, label_std=labels_std)\n",
    "\n",
    "print(\"Training complete. Model and stats saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0a7a6-9ecf-4a6f-82d1-c324d8b83168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        all_preds.append(y_pred.numpy())\n",
    "        all_targets.append(y_batch.numpy())\n",
    "\n",
    "preds = np.concatenate(all_preds).squeeze()\n",
    "targets = np.concatenate(all_targets).squeeze()\n",
    "\n",
    "# Unnormalize\n",
    "preds_denorm = preds * labels_std + labels_mean\n",
    "targets_denorm = targets * labels_std + labels_mean\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mae = mean_absolute_error(targets_denorm, preds_denorm)\n",
    "rmse = np.sqrt(mean_squared_error(targets_denorm, preds_denorm))\n",
    "r2 = r2_score(targets_denorm, preds_denorm)\n",
    "\n",
    "print(f\"Test MAE:  {mae:.2f}° latitude\")\n",
    "print(f\"Test RMSE: {rmse:.2f}° latitude\")\n",
    "print(f\"Test R² Score: {r2:.3f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(targets_denorm, preds_denorm, alpha=0.5, color=\"royalblue\", label=\"Predictions\")\n",
    "plt.plot([min(targets_denorm), max(targets_denorm)],\n",
    "         [min(targets_denorm), max(targets_denorm)],\n",
    "         'k--', label=\"Ideal (y = x)\")\n",
    "plt.xlabel(\"True Hadley Edge Latitude (°)\")\n",
    "plt.ylabel(\"Predicted Latitude (°)\")\n",
    "plt.title(\"Model Performance: Hadley Cell Ascending Edge (Test Set)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83527b39-f8ba-48b7-8838-4afe4bc34716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hadley_edge(streamfunc_2d):\n",
    "    \"\"\"\n",
    "    Predicts the Hadley cell ascending edge latitude given a new 2D stream function array.\n",
    "    Input shape should be (level, lat), i.e., (37, 721).\n",
    "    \"\"\"\n",
    "    assert streamfunc_2d.shape == (37, 721), \"Input shape must be (37, 721)\"\n",
    "\n",
    "    # Load normalization stats\n",
    "    stats = np.load(\"hadley_norm_stats.npz\")\n",
    "    input_mean = stats[\"input_mean\"]\n",
    "    input_std = stats[\"input_std\"]\n",
    "    label_mean = stats[\"label_mean\"]\n",
    "    label_std = stats[\"label_std\"]\n",
    "\n",
    "    # Normalize and reshape input\n",
    "    streamfunc_2d_norm = (streamfunc_2d - input_mean) / input_std\n",
    "    input_tensor = torch.tensor(streamfunc_2d_norm.reshape(1, -1), dtype=torch.float32)\n",
    "\n",
    "    # Load model\n",
    "    input_dim = 37 * 721\n",
    "    model = HadleyNet(input_dim)\n",
    "    model.load_state_dict(torch.load(\"hadley_model.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        pred_norm = model(input_tensor).item()\n",
    "        pred_lat = pred_norm * label_std + label_mean\n",
    "\n",
    "    return pred_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959fcce6-121a-45e8-a911-3d4947ae71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict edge for a single streamfunction\n",
    "predicted_lat=predict_hadley_edge(ds.streamfunc.sel(time=\"2020-08-01\").values)\n",
    "plt.figure(figsize=(20, 10))\n",
    "ds.streamfunc.sel(time=\"2020-08-01\").plot.contourf(yincrease=False,levels=20)\n",
    "labeled_lat = ds[\"hc_edge_shared\"].sel(time=\"2020-08-01\")\n",
    "plt.axvline(labeled_lat, color = 'orange')\n",
    "plt.axvline(predicted_lat, color = 'yellow')\n",
    "print(f\"Predicted lat = {predicted_lat:.2f}° latitude\")\n",
    "print(f\"Labeled lat = {labeled_lat:.2f}° latitude\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
